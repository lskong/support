[global]
fsid = dc749284-fda7-4f16-ae21-7b9d66393b20
# ceph 集群标识ID，必要

public_network = 10.0.0.0/24
cluster_network = 10.0.1.0/24
# public mon所在的网络，也是client连接的网络
# cluster 实际是osd数据网

mon_initial_members = cli-1
mon_host = 10.0.0.21
# moniter节点信息，必要

auth_cluster_required = cephx
auth_service_required = cephx
auth_client_required = cephx
# 集群验证，必要

mon_allow_pool_delete = true
# 允许删除存储池，默认值false，生产环境禁止开启

osd_pool_default_size = 3
osd_pool_default_min_size = 2
osd_pool_default_pg_num = 512
osd_pool_default_pgp_num = 512
# 存储池复本为3，最小为2，
# pg和pgp的大小与osd数量有关系，
# 公式为250*osd数量/复本数，再取2的n次方
# 例如：250*12/3=1000，取值512，不能大于1000

#osd crush chooseleaf type = 1
#max open files = 131072
#log file = /var/log/ceph/ceph-$name.log
#pid file = /var/run/ceph/$name.pid
#enable experimental unrecoverable data corrupting features = bluestore rocksdb

# rbd_default_features = 1
# 创建rbd时，自动根据内核选择特性，不需要在指定特性。注意内核低不适用   

# =============================================================================
[mgr]
mgr modules = dashboard
# 开启ceph仪表盘


# =============================================================================
[mon]
#debug mon = 10
#debug paxos = 0


# =============================================================================
[osd]
bluestore = true
osd objectstore = bluestore
bluestore fsck on mount = true
bluestore block create = true
bluestore block db size = 42949672960
bluestore block db create = true
bluestore block wal size = 64424509440
bluestore block wal create = true
bluestore_cache_size_ssd = 10737418240
osd heartbeat grace = 20
osd heartbeat interval = 5

#osd max write size = 512
# 默认值90，OSD一次可写入的最大值(MB)

#osd client message size cap = 2147483648
# 默认值500，客户端允许在内存中的最大数据(bytes)

#osd op threads = 16
#默认值2，并发文件系统操作数

#osd disk threads = 4
# 默认值1，OSD密集型操作例如恢复和Scrubbing时的线程

#osd map cache size = 1024
# 默认值50，保留OSD Map的缓存(MB)

#osd map cache bl size = 128
# 默认值50，OSD进程在内存中的OSD Map缓存(MB)

#osd recovery op priority = 2
# 默认值10，恢复操作优先级，取值1-63，值越高占用资源越高

#osd recovery max active = 10
# 默认值15，同一时间内活跃的恢复请求数

#osd max backfills = 4
# 默认值10，一个OSD允许的最大backfills数

#osd min pg log entries = 30000
# 默认值3000，修建PGLog是保留的最大PGLog数

#osd max pg log entries = 100000
# 默认值10000，修建PGLog是保留的最大PGLog数

#osd mon heartbeat interval = 40
# 默认值30，OSD ping一个monitor的时间间隔

#ms dispatch throttle bytes = 1048576000
# 默认值 104857600，等待派遣的最大消息数

#objecter inflight ops = 819200 
# 默认值1024，客户端流控，允许的最大未发送io请求数，超过阀值会堵塞应用io，为0表示不受限

#osd op log threshold = 50
# 默认值5，一次显示多少操作的log

#osd crush chooseleaf type = 0
# 默认值为1，CRUSH规则用到chooseleaf时的bucket的类型


# =============================================================================
[client]

#rbd cache = true
# 默认值 true，RBD缓存

#rbd cache size = 335544320
# 默认值33554432，RBD缓存大小(bytes)

#rbd cache max dirty = 134217728
# 默认值25165824，缓存为write-back时允许的最大dirty字节数(bytes)，如果为0，使用write-through

#rbd cache max dirty age = 30
# 默认值1，#在被刷新到存储盘前dirty数据存在缓存的时间(seconds)

#rbd cache writethrough until flush = false
# 默认值true  #该选项是为了兼容linux-2.6.32之前的virtio驱动，避免因为不发送flush请求，数据不回写
# 设置该参数后，librbd会以writethrough的方式执行io，直到收到第一个flush请求，才切换为writeback方式。

#rbd cache max dirty object = 2
# 默认值0，最大的Object对象数，默认为0，表示通过rbd cache size计算得到，librbd默认以4MB为单位对磁盘Image进行逻辑切分
# 每个chunk对象抽象为一个Object；librbd中以Object为单位来管理缓存，增大该值可以提升性能

#rbd cache target dirty = 235544320
# 默认值16777216，开始执行回写过程的脏数据大小，不能超过 rbd_cache_max_dirty