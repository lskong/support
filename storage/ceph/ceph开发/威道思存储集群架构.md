# 威道思存储集群架构设计

- [威道思存储集群架构设计](#威道思存储集群架构设计)
  - [两层架构](#两层架构)
    - [S3 API 层](#s3-api-层)
    - [对象存储层](#对象存储层)
  - [提升Ceph性能](#提升ceph性能)
    - [提升大文件读取写入性能](#提升大文件读取写入性能)
    - [提升小文件性能](#提升小文件性能)
      - [GC 设计](#gc-设计)
    - [ceph数据均衡](#ceph数据均衡)
      - [对象名称生成器](#对象名称生成器)
    - [存储系统分级](#存储系统分级)
      - [存储策略](#存储策略)
      - [对象数据存储策略](#对象数据存储策略)
      - [对象生命周期管理](#对象生命周期管理)
      - [自动生成迁移策略](#自动生成迁移策略)



## 两层架构

![架构](/img/4ec2515a43f14f1b8228a5fe8ed364f1.png "Magic Gardens")

### S3 API 层
负责S3 API的解析和处理。所有元数据存储在HBASE中，元数据包括bucket的信息，object的元数据（如ACL、contentType），multipart的切片信息，权限管理，BLOB Storage的权值和调度，同时所有的元数据都cache在统一的cache层。这样可以看到所有元数据都存储在HBASE中，并且有统一的cache，相比于radosgw大大提高的对元数据操作的可控性，也提高了元数据查询的速度。

### 对象存储层
可以并行的存在多个Ceph Cluster。只使用 rados_read/rados_write的API。如果其中一个ceph cluster正在做rebalance，可以把它上面所有写请求调度到其他ceph集群，减少写的压力，让rebalance迅速完成。从用户角度看，所有的写入并没有影响，只是到这个正在rebalance的ceph cluster上的读请求会慢一点儿。

这种设计使得大规模扩容也变得非常容易，比如：初期上线了5台服务器做存储，使用过程中发现容量增加很快，希望扩容到50台，但是在原ceph集群上一下添加45台新服务器，rebalance的压力太大。在我们的环境中，只要新建一个45台的ceph集群，接入我们的环境就行，可以快速无缝的灵活扩充。

===========================================================
## 提升Ceph性能

### 提升大文件读取写入性能
对于大文件，相比于radosgw每次使用512K的buf，用rados_write的API写入ceph集群，我们使用动态的buf，根据用户上传的速度的大小调整buf在(512K~1M)之间。并且使用rados striping的API提高写入的并发程度。让更多的OSD参与到大文件的写入，提高并发性能。

拆分方法如下图：

![架构](/img/62123e9a72fe42279add2584719bf4ff.png "Magic Gardens")

### 提升小文件性能
对象存储层中将小对象推送给缓存OSD, 通过离线程序异步扫描一定时间段之前的小对象, 将小对象推进消息队列（Kafka）， 消费者从消息队列拉取数据执行小对象合并流程，完成对象的生命周期转换，整体的架构图如下：

![架构](/img/lifecycle-arth.png "Magic Gardens")

条带的拼装就是不停的从 Kafka 中取数据来填充一个 Ceph 条带，我们设计了一种分级队列的拼装方法，思路是在内存维护一个 MergeList 数据结构，对大于 4M 的对象不需要合并

执行合并的示意图：

![架构](/img/small-file-merge.png "Magic Gardens")


#### GC 设计
小对象合并时条带的头初始 4k 位置标记为 header, 原生的 Ceph 提供了的对象存储是 RGW，可以采用副本和 EC 模式，但粒度都是单对象级别的；即如果我们采用 Append 方式追加写到同一个 Ceph 对象，当该 Ceph 对象中部分小对象被删除形成空洞后，Ceph 在底层没有提供垃圾回收的机制；引入小对象合并写特性后, 我们可以将一个条带中的 header 信息集中存储在条带的最开始的 4KB 位置处, 如下图：

![架构](/img/image2020-7-image2020-7-14-2011_28_1.png "Magic Gardens")

当 GC 程序需要反序列化 Ceph 块上的对象信息时，只需读取首个 osd0 上的 meta 部分，按照 header 格式将对象信息反序列化，只需要一个大 IO 就可以读取完毕。

### ceph数据均衡

Ceph的一大设计是其CRUSH算法，不需要查表，仅通过计算得到 Rados 对象的存储位置。这省去了元数据表的消耗，但也带来了问题，即数据分布不均衡，导致整体容量利用率往往只能维持在80%左右，这是一种巨大的浪费。虽然 Ceph 在较新的版本中提供了一定的自均衡功能，然而其会引发大量的数据迁移，代价依然不小。

这个问题的根本原因在于 Ceph 中映射关系完全靠固定的计算得到。当用户选定好对象名（算法主要变化的入参就是对象名称）后，其存放位置就已经基本确定下来；而用户选择什么样的名称是存储系统不可控的。

#### 对象名称生成器
我司在实现对象合并功能时，使用 HBASE 记录了存储对象到 Rados 对象的映射关系；这相当于在业务数据和 Ceph 系统之间增加了一层映射隔离，带来的好处是我们可以自主地选择 Rados 对象名称，从而控制数据存放的位置。因此，在 网关入口 中我们加入了对象名称生成功能，提前准备好合适的对象名，确保该对象能尽可能地写到使用率较低的硬盘上。另外，对于那些已经达到 full ratio 阈值的硬盘，可以选择将其过滤，然后继续往剩下的盘中写入。使用此项硬盘容量平均使用率可以提升到 97%左右，大大节省了成本。

======================================================================

### 存储系统分级
在存储集群当中，出于对访问性能、成本等因素的考虑，我们可能会同时引入 SSD 和 HDD。在这种情况下，如果不进行存储分级，就可能会导致某些对访问性能要求不高的数据，或是归档数据，被存储在 SSD 中，而某些对访问性能要求较高的数据则被存储在了 HDD 中，这无疑会影响数据的访问性能，同时也提高了数据的存储成本。

#### 存储策略
3 副本
2 副本
Erasure Code

有的数据对可靠性要求很高，我们将其以三副本的形式进行存储。有的数据可靠性要求没那么高，我们将其以两副本的形式进行存储，节省存储空间

#### 对象数据存储策略
我们在 Ceph 对象存储中引入了 object storage class 的概念，对存储池的概念进行了更高程度的抽象，不仅可以按照当前 Ceph 对象存储支持，同时：

    ◦ 可以按照不同的存储介质来划分存储池 (HDD/SSD)；
    ◦ 可以按照不同的存储策略(数据冗余策略)来划分存储池 (2x Replication/ 3x Replication/ Erasure Code)；
    ◦ 可以把外部存储 (包括外部公有云存储、私有云存储) 抽象为存储池；

将 RGW zone 的 placement rule 的作用范围进行了细粒度化的处理，使其作用到对象级别，实现了对象级别的存储分级, 即使是同一个存储桶中，不同的对象数据也可以保存在不同的存储池中。

#### 对象生命周期管理
在 Ceph RGW 现有 LC 实现的基础上，我们对 RGW LC 的处理逻辑进行了扩展，实现了 LC 迁移功能，支持通过对象生命周期管理，将对象数据迁移到其他存储类别 storage class 中，例如支持从 SSD 迁移到 HDD，从 3 副本池迁移到 2 副本池，从副本池迁移到纠删码池，从 Ceph 集群中迁移到外部公有云存储等等，从而实现了完整的对象生命周期管理。

相较于 RGW 的 Cloud Sync 功能，通过配置 LC 迁移规则将 Ceph 集群中的对象数据迁移到外部云存储具有如下优点：

      1. 操作的粒度更细，可以直接以对象为单位，对数据进行操作；
      2. 时间可控，可以通过在 LC 规则当中对操作生效的时间进行配置指定，人为控制数据迁移的时间，时间可控性更强；

至此我们已经在 Ceph 对象存储的基础上，实现了一套完整的、全粒度支持的数据迁移处理机制，从 zone 级、到 bucket 级、再到 object 级、基本可以覆盖所有应用场景的常见需求。

#### 自动生成迁移策略

**存储桶日志**
存储桶日志是用于记录追踪对某一特定存储桶的操作和访问的功能特性。存储桶日志的每条日志记录都记录了一次对相应存储桶的操作访问请求的细节，例如请求的发起者、存储桶名字、请求时间、请求的操作、返回的状态码等等。

**自动生成迁移策略**
根据存储桶日志中的操作记录、以及可配置的标尺参数，对存储桶中的对象数据的热度进行分析，并按照分析结果自动生成迁移策略，对对象数据进行管理。
