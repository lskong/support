
- [基于ceph的高性能存储产品设计及源码分析](#基于ceph的高性能存储产品设计及源码分析)
  - [Ceph面临的挑战](#ceph面临的挑战)
  - [通用场景](#通用场景)
    - [产品简介](#产品简介)
    - [产品优势](#产品优势)
    - [方案亮点](#方案亮点)
      - [高速网络支持](#高速网络支持)
        - [Reactor模型](#reactor模型)
        - [Async模块](#async模块)
        - [RDMA](#rdma)
        - [性能优化工作](#性能优化工作)
        - [从以上测试结果来看，主要有以下结论：](#从以上测试结果来看主要有以下结论)
  - [威道思的存储集群架构](#威道思的存储集群架构)
    - [S3 API layer](#s3-api-layer)
    - [BLOB Storage层](#blob-storage层)
    - [在Ceph层提升性能](#在ceph层提升性能)
      - [提升大文件读取写入性能](#提升大文件读取写入性能)
      - [提升小文件性能](#提升小文件性能)
        - [三副本模式ceph提升小文件性能](#三副本模式ceph提升小文件性能)
        - [EC模式ceph基于对小对象进行聚合优化提升性能](#ec模式ceph基于对小对象进行聚合优化提升性能)
    - [ceph数据均衡](#ceph数据均衡)
      - [对象名称生成器](#对象名称生成器)
  - [存储系统分级](#存储系统分级)
    - [存储策略](#存储策略)
    - [RGW 数据存放规则](#rgw-数据存放规则)
    - [对象数据存储策略](#对象数据存储策略)
    - [对象生命周期管理](#对象生命周期管理)
      - [实现完整的对象生命周期管理](#实现完整的对象生命周期管理)
    - [自动生成迁移策略](#自动生成迁移策略)
  - [高性能计算场景](#高性能计算场景)
    - [背景](#背景)
    - [解决方案](#解决方案)
    - [方案亮点](#方案亮点-1)
  - [海量小文件场景](#海量小文件场景)
    - [背景](#背景-1)
    - [解决方案](#解决方案-1)
    - [方案亮点](#方案亮点-2)
  - [大数据处理场景](#大数据处理场景)
    - [背景](#背景-2)
    - [解决方案](#解决方案-2)
    - [方案亮点](#方案亮点-3)
  - [视频监控场景](#视频监控场景)
    - [背景](#背景-3)
    - [解决方案](#解决方案-3)
    - [方案亮点](#方案亮点-4)
  - [医疗影像场景](#医疗影像场景)
    - [背景](#背景-4)
    - [解决方案](#解决方案-4)
    - [方案亮点](#方案亮点-5)
  - [媒体融合场景](#媒体融合场景)
    - [背景](#背景-5)
    - [解决方案](#解决方案-5)
    - [方案亮点](#方案亮点-6)
  - [高清制作场景](#高清制作场景)
    - [背景](#背景-6)
    - [解决方案](#解决方案-6)
    - [方案亮点](#方案亮点-7)
  - [媒资管理场景](#媒资管理场景)
    - [背景](#背景-7)
    - [解决方案](#解决方案-7)
    - [方案亮点](#方案亮点-8)
  - [集群渲染场景](#集群渲染场景)
    - [背景](#背景-8)
    - [解决方案](#解决方案-8)
    - [方案亮点](#方案亮点-9)



# 基于ceph的高性能存储产品设计及源码分析

## Ceph面临的挑战
１)Ceph底层采用定长的对象存储,为了保证对象级别的原子性,底层存储引擎的写放大问题严重影响了性能.(CephFS的特性)
２)Ceph的数据分布算法 CRUSH 在实际环境中存在一些问题,包括扩容时数据迁移不可控、数据分布不均衡等.这些问题影响了 Ceph性能的稳定性.
３)Ceph对新型存储介质的支持较差.在使用高速存储介质时, 软件造成的时延比硬件导致的时延高出数十倍. 社区也在开发面向新型存储介质的存储引擎.
４)Ceph的架构复杂,抽象层次多,时延较大.虽然 Ceph采用面向对象的设计思想,但其代码内对象间的耦合严重,导致不同版本间的接口不兼容.针对不同版本的性能优化技术
和方法也互相不兼容.
５)Ceph是一个通用的分布式存储系统,可应用于云计算、大数据和高性能计算等领域.针对不同的访问负载特征,Ceph还有较大的性能提升和优化空间.

## 通用场景

### 产品简介
本公司基于ceph的高性能存储是面向海量数据的新型分布式存储系统，构建于低成本的通用硬件平台之上，依靠其rados核心系统管理所有存储资源，采用横向弹性扩展架构，为用户提供可通过多种接口访问的单一文件系统服务，具有高可扩展、高性能、高可用、易管理等特性。

**弹性集群架构**
采用横向扩展的集群架构，系统容量与性能可随存储节点数目线性增长，突破扩展瓶颈。存储节点随用随加，可按需在线完成系统扩展，并自动均衡数据。

**通用硬件平台**
基于通用多盘位x86服务器、普通磁盘及主流网络构建，应用软件整合所有存储资源，并保证数据可靠性及服务可用性，硬件成本相比传统存储系统大幅降低。

**数据优化管理**
文件数据与元数据分离，各自优化管理。元数据全内存存取优化，采用share-nothing集群机制。数据采用智能副本策略，跨节点冗余存放，读写实时校验。

### 产品优势
**性能卓越**
系统性能随节点数线性扩展，可达TB级带宽及百万级IOPS。基于内存优化及动态分区的元数据服务集群，元数据性能优异。多方位SSD磁盘加速支持，提升热点数据及小文件性能表现。优化流控及智能调度，确保高并发、超负荷下，性能稳定一致。特有客户端磁盘缓存功能，减少后端系统开销，降低访问延迟。

**接口丰富**
文件/对象/块三大接口统一支持，全面满足各类存储需求。专有本地客户端（Windows/Mac/Linux)，文件高效并发访问。传统主流 CIFS/NFS/FTP/iSCSI 接口支持，跨平台完美兼容。API编程接口（C++/Java/Python)，克服传统接口的诸多局限。HDFS接口支持，无缝对接Hadoop平台，助力大数据处理分析。

**稳定可靠**
高可用集群架构，高可靠服务设计，系统无单点故障。基于策略跨节点冗余分布及校验存取，数据安全可靠。故障自动切换，数据自动恢复，2TB/h数据恢复速度。真实大规模、高负载生产环境下，连续2600+天稳定运行。

**轻松管理**
快速集群部署支持，分钟级系统扩展流程。图形化简洁管理界面，一站式系统管理运维。内建多种自动管理运维流程，极少人工参与。完备的系统日志、监控、图表、统计及告警支持。

### 方案亮点

#### 高速网络支持
使用RDMA优化网络性能瓶颈，基于async网络通信模块进行优化，实现在源代码src/msg的目录下，该目录主要包括Messenger、Connection、Message、Dispatch等类，这些类定义了网络通信的框架与接口。

##### Reactor模型
为了处理高并发的网络I/O流，async模块采用了Reactor模型。在Reactor中，每一种handler会出处理一种event。这里会有一个全局的管理者selector，我们需要把channel注册感兴趣的事件，那么这个selector就会不断在channel上检测是否有该类型的事件发生，如果没有，那么主线程就会被阻塞，否则就会调用相应的事件处理函数即handler来处理。

![Reactor模型](/img/20200713103137218.png "Magic Gardens")

**Reactor模型原理**
![Reactor模型原理](/img/20200713103137382.png "Magic Gardens")

**Reactor模型主要组件**

**Reactor模型的优点**
响应快，不必为单个同步时间所阻塞，虽然Reactor本身依然是同步的；编程相对简单，可以最大程度的避免复杂的多线程及同步问题，并且避免了多线程/进程的切换开销； 可以方便的通过增加Reactor实例个数来充分利用CPU资源；reactor框架本身与具体事件处理逻辑无关，具有很高的复用性；

**Reactor模型的缺点**
相比传统的简单模型，Reactor增加了一定的复杂性，因而有一定的门槛，并且不易于调试； Reactor模式需要底层的Synchronous Event Demultiplexer支持，比如Java中的Selector支持，操作系统的select系统调用支持，如果要自己实现Synchronous Event Demultiplexer可能不会有那么高效；Reactor模式在IO读写数据时还是在同一个线程中实现的，即使使用多个Reactor机制的情况下，那些共享一个Reactor的Channel如果出现一个长时间的数据读写，会影响这个Reactor中其他Channel的相应时间，比如在大文件传输时，IO操作就会影响其他Client的相应时间，因而对这种操作，使用传统的Thread-Per-Connection或许是一个更好的选择，或则此时使用Proactor模式。

**有限状态机模型**
有限状态机(Finite State Machine, FSM)，是表示有限个状态以及在这些状态之间的转移和动作等行为的数学模型。
FSM模型把模型的多状态、多状态间的转换条件解耦；可以使维护变得容易，代码也更加具有可读性。
AsyncConnection连接建立过程中地状态迁移图参阅附录(Ⅰ)。

##### Async模块
**Async工作原理**
![Async工作原理](/img/20200713103136360.png "Magic Gardens")

**Async主要组件**

| 组件名 |  描述 |
| :-----| :---- |
| AsyncMessenger | 管理网络连接 |
| AsyncConnection | 网路通信连接，定义网络通信应用层协议 |
| NetworkStack | 管理Worker对象及其对应地线程 |
| Worker | 网络I/O流处理单元，每个Worker对应一个工作线程
| ServerSocket/ServerSocketImpl | C/S模式监听套接字，向上屏蔽了各种不同的网络编程接口|
| ConnectedSocket/ConnectedSocketImpl | C/S模式连接套接字，向上屏蔽了各种不同的网络编程接口 |
| EventCenter | 事件分发器，负责事件注册、事件分发 |
| EventCallback | 当对应的事件发生时，由EventCenter负责回调 |
| EventEpoll | 对epoll进行封装，轮询网络I/O事件 |

##### RDMA
想从根本上解决CPU参与网络传输的低效问题，就要更多地借助专用芯片的能力。

RDMA（Remote Direct Memory Access），可以简单理解为网卡完全绕过CPU实现两个服务器之间的内存数据交换。其作为一种硬件实现的网络传输技术，可以大幅提升网络传输效率，帮助网络IO密集的业务（比如分布式存储、分布式数据库等）获得更低的时延以及更高的吞吐。一旦应用程序分配好资源，其可以直接把要发送的数据所在的内存地址和长度信息交给网卡。网卡从内存中拉取数据，由硬件完成报文封装，然后发送给对应的接收端。接收端收到RDMA报文后，直接由硬件解封装，取出数据后，直接放在应用程序预先指定的内存位置。

由于整个IO过程无需CPU参与，无需操作系统内核参与，没有系统调用，没有中断，也无需内存拷贝，因此RDMA网络传输可以做到极高的性能。在极限benchmark测试中，RDMA的时延可以做到1us级别，而吞吐甚至可以达到200G。

具体来说，RDMA技术的应用要借助支持RDMA功能的网卡以及相应的驱动程序。

**但是RDMA当前还无法像TCP那样通用，有一些基础设施层面的限制需要注意：**
RDMA需要网卡硬件支持。常见的万兆网卡一般不支持这项技术。
RDMA的正常使用有赖于物理网络支持。

**RDMA三种不同的硬件实现**
目前，有三种RDMA协议的实现：Infiniband、RoCE、iWARP。由于RoCE具备明显性能和成本优势，将逐渐成为市场主流。

![RDMA三种不同的硬件实现](/img/20200713103136978.jpeg "Magic Gardens")

**软件栈对比**
![软件栈对比](/img/2020071310313xos31.png "Magic Gardens")

[^_^]:
    | 左对齐 | 右对齐 | 居中对齐 |
    | :-----| ----: | :----: |
    | 单元格 | 单元格 | 单元格 |
    | 单元格 | 单元格 | 单元格 |

| 组件名 |  描述 |
| :-----| :---- |
| Device/DeviceList | 抽象RDMA网卡，根据icfs.conf配置网卡参数 |
| Infiniband | 封装IB |
| Verbs | 网络编程接口及组件 |
| RDMAConnectedSocketImpl | 仿socket连接套接字，采用伪fd实现网络I/O流的数据读写 |
| RDMAConnTCP | 为RDMAConnectedSocketImpl服务，利用利用TCP/IP协议建立RDMA连接 |
| RDMAServerSocketImpl | 仿socket服务套接字，定义服务接口 |
| RDMAServerConnTCP | 实现RDMAServerSocketImpl接口，利用TCP/IP协议建立RDMA连接 |
| RDMADispatcher | 轮询RDMA网络I/O流可读事件，将网络I/O流可读数据分发到对应RDMAConnectedSocketImpl 轮询RDMA网络I/O流可写事件，将网络I/O流可写数据分发到某个RDMAWorker|
| RDMAWorker | 网络I/O流处理单元，每个RDMAWorker对应一个工作线程|
| RDMAStack | 管理RDMAWorker对象及其对应地线程 |

##### 性能优化工作

**RoCE网络通信的实现**
由于Infiniband与RoCE网络开发采用相同上层Verbs API，因此，IB网络通信代码可以完全在RoCE硬件上运行，整个代码几乎不需要改动。

**性能测试平台开发**
为了能够对网络模块通信性能及优化效果进行定性、定量地深入研究，需要一套相对独立地RDMA网络通信性能测试工具。

**aysnc_server/async_client**
async_client向async_server发送MSG_DATA_PING类型地数据包，async_server当受到2000个数据包之后会自动关闭连接，async_client监测到async_server端关闭之后，async_client会停止发送数据包，同时输出网络通信性能地统计信息。

**async_server命令参数**
--addr X ip to listen
--port X port to bind

**async_client命令参数：**
--msgs X number of msg to transport
--dszie X size of each msg to transport
--addr X ip of the server
--port X port of the server

这种测试工具其实是利用async_server端连接关闭作为消息数据包发送结束的标志，因为async_client感知到async_server连接关闭需要一定的时间，从而导致不能够准确地测试网络性能。

**ceph_perf_msgr_server/ceph_perf_msgr_client**
采用“请求-应答”模式，具体实现上与实际的OSD业务通信流程比较相似，因此可以较好的反映网络通信性能。

client向server端发送指定数量的MOSDOp消息，server端对于收到的每个MOSDOp消息，都会向client端发送MOSDOpReply消息。

**RoCE event**
![event](/img/2020071310313xos32.png "Magic Gardens")

**RoCE polling**
![polling](/img/2020071310313xos33.png "Magic Gardens")


##### 从以上测试结果来看，主要有以下结论：
无论采用polling还是event轮询模式，网络性能几乎一样。
随着连接数的增大，网络性能逐渐达到性能瓶颈，最大IOPS为15万左右。
当连接数增大到一定程度，IOPS维持在10万左右。


[^_^]:
    **QueuePair发送队列**
    通过读取ms_async_rdma_receive_buffer与ms_async_rdma_send_buffers来配置注册内存大小，在Device::create_queue_pair()中，会根据ms_async_rdma_send_buffers来创建QueuePair.即
    ```C++
            Infiniband::QueuePair* Infiniband::create_queue_pair(CephContext *cct, CompletionQueue *tx,
                CompletionQueue* rx, ibv_qp_type type, struct rdma_cm_id *cm_id)
            {
                Infiniband::QueuePair *qp = new QueuePair(
                    cct, *this, type, ib_physical_port, srq, tx, rx, tx_queue_len, rx_queue_len, cm_id);
                if (qp->init()) {
                    delete qp;
                    return NULL;
                }
                return qp;
            }
    ```

    **但是ms_async_rdma_send_buffers设置较大会导致创建QueuePair失败，需要独立地设置注册内存以及QueuePair的创建**

    ```C++
            Infiniband::QueuePair* Infiniband::create_queue_pair(CephContext *cct, CompletionQueue *tx,
                CompletionQueue* rx, ibv_qp_type type, struct rdma_cm_id *cm_id)
            {
                /*
                Infiniband::QueuePair *qp = new QueuePair(
                    cct, *this, type, ib_physical_port, srq, tx, rx, tx_queue_len, rx_queue_len, cm_id);
                */

                static uint32_t max_qp_wr = device->device_attr.max_qp_wr - 1;
                Infiniband::QueuePair *qp = new QueuePair(
                    cct, *this, type, ib_physical_port, srq, tx, rx, max_qp_wr, rx_queue_len, cm_id);
                if (qp->init()) {
                    delete qp;
                    return NULL;
                }
                return qp;
            }
    ```

    **经过修改之后，达到了以下效果**
    注册内存buffer大小(ms_async_rdma_buffer_size)可由4096增加到131072
    注册内存buffer数量(ms_async_rdma_send_buffers/ms_async_receive_buffers)可由1024增加到10240
    解决了1M大小数据块测试过程中数据断流问题
    
    这个问题我看了一下， 15.2.10源码本身就对send_buffer的大小有限制， 最大不准超过设备max大小， 所以这个问题应该不存在在15.2.10版本中， 他用的源码是10.2.3


## 威道思的存储集群架构

架构从整体看，分为2层

![架构](/img/4ec2515a43f14f1b8228a5fe8ed364ff.png "Magic Gardens")

### S3 API layer
负责S3 API的解析和处理。所有元数据存储在HBASE中，元数据包括bucket的信息，object的元数据（如ACL、contentType），multipart的切片信息，权限管理，BLOB Storage的权值和调度，同时所有的元数据都cache在统一的cache层。这样可以看到所有元数据都存储在HBASE中，并且有统一的cache，相比于radosgw大大提高的对元数据操作的可控性，也提高了元数据查询的速度。

### BLOB Storage层
可以并行的存在多个Ceph Cluster。只使用 rados_read/rados_write的API。如果其中一个ceph cluster正在做rebalance，可以把它上面所有写请求调度到其他ceph集群，减少写的压力，让rebalance迅速完成。从用户角度看，所有的写入并没有影响，只是到这个正在rebalance的ceph cluster上的读请求会慢一点儿。

这种设计使得大规模扩容也变得非常容易，比如：初期上线了5台服务器做存储，使用过程中发现容量增加很快，希望扩容到50台，但是在原ceph集群上一下添加45台新服务器，rebalance的压力太大。在我们的环境中，只要新建一个45台的ceph集群，接入我们的环境就行，可以快速无缝的灵活扩充。

### 在Ceph层提升性能

####  提升大文件读取写入性能
对于大文件，相比于radosgw每次使用512K的buf，用rados_write的API写入ceph集群，我们使用动态的buf，根据用户上传的速度的大小调整buf在(512K~1M)之间。并且使用rados striping的API提高写入的并发程度。让更多的OSD参与到大文件的写入，提高并发性能。

#### 提升小文件性能

##### 三副本模式ceph提升小文件性能
1. 用kvstore/bluestore, 底层用rocksdb +	bluefs
2. PATCH:	rados增加新FLAG:LIBRADOS_OP_FLAG_FADVISE_NEWOBJ, 在ceph的写入之前, 不再尝试读取文件,	CPU占用降低了50%
3. 增加rocksdb的sst文件大小

##### EC模式ceph基于对小对象进行聚合优化提升性能
选择 EC 的首要出发点是降低单位存储成本，用于存储低频数据，比起传统的 3 副本存储方式，选择 8 + 3 EC 策略，存储副本数变为 1.375( 11 / 8)，节约了 1.625 的存储副本
同时相比三副本的存储方式（不管是客户端直接写 3 副本，还是由复制组中的 master 节点接收数据后写两个 secondary 节点）写入流量减少将近 1 倍

从小对象业务场景的对象大小统计分布情况看，80% 的对象大小都在 0 ~ 128KB 范围内，小对象的高占比是我们可以做对象合并的基础前提。

**而从对象大小的角度来看：**
小对象：一般是低于 128KB 的，比如图片、文本，这一类对象，要求是 TPS
大对象：一般是大于 128KB 的文件，更重视整体的带宽吞吐

**我们的存储集群的性能指标要求：**
整体 TPS：30000+，读写占比符合28定律：20%写请求，80%读请求
小对象 latency：读 < 100ms，写 < 200ms（90分位值）
大对象吞吐：读 > 10GBps，写 > 1GBps（总体带宽）

由于 EC 必须要条带对齐，对小对象来说，独占条带会严重降低有效数据利用率，所以优化目标是小对象合并，通过合并对象写入，降低写 IOPS。

我们将条带大小由 64KB 调整为 1MB，则单个 osd 的 data chunk 变成 128KB，而我们 80% 的对象都是小对象，即单个对象可以直接存放在单个 osd 的 chu中；如果某个 osd 故障仍然可以通过条带中的其他 osd 通过 EC 计算恢复。只是此时用于 EC 恢复的数据是其他对象

小对象合并的过程不能用作用户的在线写 API，用户的 PUT 操作都是对象级别的，需要实时响应 PUT 状态，而上述模型需要多个小对象拼凑出一个完整的条带，需要一定的时间差，而极端情况下可能根本拼凑出一个条带；为此我们需要转变思路，引入第二个优化点离线写在线读。

**离线写在线读**
简言之，即我们不能根据用户的实时上传对象拼装 Ceph EC 条带，用户 PUT 上传仍然写三副本存储引擎，通过离线程序异步的扫描一定时间段之前的用户数据，执行条带拼装流程，拼装好的条带再写入 EC

    写请求通过先写三副本，再通过离线中转程序迁移至 Ceph，实现离线合并写
    读请求因为数据已经转入Ceph ，并且做了小对象合并，可以直接读取 Ceph

我们通过 自研服务 扫描已经上传到三副本的对象推送给消息队列（Kafka）， 消费者从消息队列拉取数据执行小对象合并流程，完成对象的生命周期转换，整体的架构图如下：

    Kafka 消息队列解耦了小对象合并的数据流，同时也解耦非合并写和对象的垃圾回收等对象周期生命的数据流
    Kafka 的消息分布按照业务桶（Bucket）做 hash 聚合，保障相同桶的数据分配到相同的 Kafka partition 中
    根据 Kafka 消费模型，一个 Kafka Partition 同时只能被一个消费者消费，保障相同桶的对象尽量被相同的消费者聚合合并

条带的拼装是一个静态的过程，不停的从 Kafka 中取数据来填充一个 Ceph 条带，利用一种分级队列的拼装方法，在内存维护一个 MergeList 数据结构，对大于 4M 的对象不需要合并, 根据对象的大小来决定其在一个 Ceph 条带中的分布位置，此时并没有发生真实的数据读写，只有当条带拼装成功后将该条带按批处理的方式提交给 submit 线程处理，submit 线程从三副本读取内容写 Ceph EC 再执行对象的元数据信息变更

GC 设计
小对象合并时条带的头初始 4k 位置标记为 header, 原生的 Ceph 提供了的对象存储是 RGW，可以采用副本和 EC 模式，但粒度都是单对象级别的；即如果我们采用 Append 方式追加写到同一个 Ceph 对象，当该 Ceph 对象中部分小对象被删除形成空洞后，Ceph 在底层没有提供垃圾回收的机制；引入小对象合并写特性后, 我们可以将一个条带中的 header 信息集中存储在条带的最开始的 4KB 位置处, 当 GC 程序需要反序列化 Ceph 块上的对象信息时，只需要读取首个 osd0 上的 meta 部分，在代码实现中我们是读取了 osd0 的前 4M 内容( Ceph 块大小为 32M，条带大小为1M，一个 Ceph 块上有 32 个条带)，按照 header 格式将对象信息反序列化，而这 4M 内容在 osd0 上是连续存储的，只需要一个大 IO 就可以读取完毕。

### ceph数据均衡

Ceph 的一大开创性设计是其CRUSH算法，可以不需要查表，仅通过计算得到 Rados 对象的存储位置。这虽然省去了元数据表的消耗，但也为 Ceph 带来了经常被诟病的问题，即数据分布不均衡（因为完全依赖于哈希函数的随机性）。在 NOS 原有的存储引擎中，互为副本的硬盘成对使用，每块硬盘仅需保留数十 GiB 左右的空间作缓冲即可，对于对象存储中动辄上 10TB 的硬盘这完全可以接受，其最终的容量使用率可以达到 99%甚至以上。而 Ceph 在一个存储池中某块盘达到 full ratio（默认为 95%）时就会限制整个池的业务写入（因为无法预测到新数据是否会哈希到这块盘上），这时同一个池中使用率最低的硬盘可能仅用了 60% ～ 70%，导致整体容量利用率往往只能维持在 80%左右，这是一种巨大的浪费。虽然 Ceph 在较新的版本中提供了一定的自均衡功能，然而其会引发大量的数据迁移，代价依然不小。

这个问题的根本原因在于 Ceph 中映射关系完全靠固定的计算得到。当用户选定好对象名（算法主要变化的入参就是对象名称）后，其存放位置就已经基本确定下来；而用户选择什么样的名称是存储系统不可控的。

#### 对象名称生成器
我司在实现对象合并功能时，使用 HBASE 记录了存储对象到 Rados 对象的映射关系；这相当于在业务数据和 Ceph 系统之间增加了一层映射隔离，带来的好处是我们可以自主地选择 Rados 对象名称，从而控制数据存放的位置。因此，在 网关入口 中我们加入了对象名称生成功能，提前准备好合适的对象名，确保该对象能尽可能地写到使用率较低的硬盘上。另外，对于那些已经达到 full ratio 阈值的硬盘，可以选择将其过滤，然后继续往剩下的盘中写入。使用此项硬盘容量平均使用率可以提升到 97%左右，大大节省了成本。

## 存储系统分级
在存储集群当中，出于对访问性能、成本等因素的考虑，我们可能会同时引入 SSD 和 HDD。在这种情况下，如果不进行存储分级，就可能会导致某些对访问性能要求不高的数据，或是归档数据，被存储在 SSD 中，而某些对访问性能要求较高的数据则被存储在了 HDD 中，这无疑会影响数据的访问性能，同时也提高了数据的存储成本。

### 存储策略
3 副本
2 副本
Erasure Code

有的数据对可靠性要求很高，我们将其以三副本的形式进行存储。有的数据可靠性要求没那么高，我们将其以两副本的形式进行存储，节省存储空间

### RGW 数据存放规则
本身在 RGW 中，是存在 placement rule概念的，即数据的存放规则。可以在 placement rule 中定义存储桶索引数据存放的存储池 index pool， 对象数据存放的存储池 data pool，以及通过 Multipart 上传大文件时临时数据存放的存储池 data extra pool。

因为 placement rule 是针对所使用的存储池进行定义，而存储池是位于 zone 之下的概念，所以在 RGW 中将 placement rule 作为一个 zone 级别的配置， 其作用影响的粒度为存储桶级，即可以指定存储桶所使用的 placement rule ，那所有上传到该存储桶中的对象数据都会按照该存储桶的 placement rule 定义的存放规则进行存放。用户可以通过为不同的存储桶配置不同的 placement rule 来实现将不同存储桶中的对象数据存放在不同的存储介质中或是使用不同的存储策略。

*然而，存储桶级的数据存放规则，显然不够灵活，无法满足某些应用场景的需求。*

### 对象数据存储策略
Storage Class 这一概念，本身是 AWS S3 中的一个重要的特性。在 S3 中，每个对象都具有 “storage-class” 这一属性，用于定义该对象数据的存储策略。 在 S3 中 Storage Class 特性支持如下几个预定义的存储策略：

    STANDARD针对频繁访问数据；
    STANDARD_IA用于不频繁访问但在需要时也要求快速访问的数据；
    ONEZONE_IA用于不频繁访问但在需要时也要求快速访问的数据。 其他 Amazon 对象存储类将数据存储在至少三个可用区 (AZ) 中，而 S3 One Zone-IA 将数据存储在单个可用区中；
    REDUCED_REDUNDANCY主要是针对一些对存储可靠性要求不高的数据，通过减少数据存储的副本数，来降低存储成本；

结合上面介绍的分布式存储系统对存储分级的需求，以及当前 RGW 中所支持的 data placement rule 的机制，我们在 Ceph 对象存储中引入了 object storage class 的概念，对存储池的概念进行了更高程度的抽象，不仅可以按照当前 Ceph 对象存储支持，同时：

    ◦ 可以按照不同的存储介质来划分存储池 (HDD/SSD)；
    ◦ 可以按照不同的存储策略(数据冗余策略)来划分存储池 (2x Replication/ 3x Replication/ Erasure Code)；
    ◦ 可以把外部存储 (包括外部公有云存储、私有云存储) 抽象为存储池；

将 RGW zone 的 placement rule 的作用范围进行了细粒度化的处理，使其作用到对象级别，实现了对象级别的存储分级, 即使是同一个存储桶中，不同的对象数据也可以保存在不同的存储池中。

### 对象生命周期管理
Ceph RGW 对象存储实际也支持 LC 对象生命周期管理。但是因为 RGW 本身并不支持 object storage class / placement rule，因此其对象生命周期管理目前只支持 Expiration actions 过期删除处理。

#### 实现完整的对象生命周期管理
基于上面实现的 Object Storage Class，在 RGW 现有 LC 实现的基础上，我们对 RGW LC 的处理逻辑进行了扩展，实现了 LC 迁移功能，支持通过对象生命周期管理，将对象数据迁移到其他存储类别 storage class 中，例如支持从 SSD 迁移到 HDD，从 3 副本池迁移到 2 副本池，从副 本池迁移到纠删码池，从 Ceph 集群中迁移到外部 Ufile 公有云存储等等，从而实现了完整的对象生命周期管理。

• 支持标准的 AWS S3 Object Lifecycle Management 的相关接口。

• 由上所述，我们实现的 Storage Class 功能支持将外部存储指定为一个存储类别的，因此支持通过配置存储桶的 LC 规则将该存储桶中的某一特定对象集迁移到外部存储中，如 UFile、S3 等等。

相较于 RGW 的 Cloud Sync 功能，通过配置 LC 迁移规则将 Ceph 集群中的对象数据迁移到外部云存储具有如下优点：

      1. 操作的粒度更细，可以直接以对象为单位，对数据进行操作；
      2. 时间可控，可以通过在 LC 规则当中对操作生效的时间进行配置指定，人为控制数据迁移的时间，时间可控性更强；

至此我们已经在 Ceph 对象存储的基础上，实现了一套完整的、全粒度支持的数据迁移处理机制，从 zone 级、到 bucket 级、再到 object 级、基本可以覆盖所有应用场景的常见需求。

### 自动生成迁移策略

**存储桶日志**
存储桶日志是用于记录追踪对某一特定存储桶的操作和访问的功能特性。存储桶日志的每条日志记录都记录了一次对相应存储桶的操作访问请求的细节，例如请求的发起者、存储桶名字、请求时间、请求的操作、返回的状态码等等。

**自动生成迁移策略**
根据存储桶日志中的操作记录、以及可配置的标尺参数，对存储桶中的对象数据的热度进行分析，并按照分析结果自动生成迁移策略，对对象数据进行管理。

## 高性能计算场景

### 背景
**性能要求极高**
终端设备发展使得采集数据量成倍增加，存储系统性能首先需要满足极高的数据采样速率要求；高性能计算的规模通常较大，海量计算作业并发执行，极高的数据并发存取速率使存储系统面临更大挑战。

**数据总量巨大**
更高精度、更大尺度的要求致使计算数据量指数级增长，二次采样数据从GB级快速攀升至TB级。计算过程中持续产生大量中间结果及检查点数据，需要PB级存储容量才能满足计算需求。

**计算模式复杂**
不同的应用数据采集方式、存储方式、访问模式差别较大，传统计算与存储分离部署的高性能计算平台方案，很难高效地支持多种类型应用。存储系统的访问接口固化，无法有效实施应用级优化。

**成本问题突出**
传统高性能计算环境多采用高端存储设备，配置专用存储硬件以满足超高性能需求，某些功能组件还需另购软件授权，存储系统投入极高；同时，限于体系架构，系统扩展成本也难以控制。

### 解决方案
**弹性扩展架构**
模块化Scale-out架构，容量和性能均可通过节点堆叠而线性扩展，系统节点数目可在线平滑扩充至数千台，支持EB级的单一存储空间；采用存储即计算的融合部署架构，突破性能扩展瓶颈，可提供数百GB/s的聚合存储带宽。

**突破性能瓶颈**
系统聚合性能随节点数目线性扩展，完美应对高性能计算中常见的大规模并发数据I/O；单流读写性能优化，多磁盘优化调度、前后端SSD缓存，元数据内存存储等多种优化方法，进一步提升系统性能表现。

**应用特性优化**
弹性部署方式，可根据应用特征构建计算/存储分离、融合、混合部署的存储计算平台；提供专有的Posix客户端及CIFS/NFS等传统网络文件服务接口，无缝对接多种应用；提供多种语言的编程接口，灵活支持应用级存储优化

**通用硬件平台**
采用普通商用服务器搭建，支持多种主流互连网络，支持通用文件服务接口，无需昂贵专有设备与软件即可构建高效存储系统；支持高速SSD与低成本的大容量SATA/SAS磁盘混插，兼顾性能与成本，打破制约系统扩展的成本限制。

### 方案亮点
**异构架构支持**
支持存储与计算分离的架构设计， 基于Nvme of协议使得远端高速存储设备性能损耗非常的小使得计算与存储异构部署成为可能，硬件资源独立避免复杂的资源竞争与资源调度, 存算分离架构的一大好处就是具有足够的灵活性，当用户规模越来越大之后，其对于灵活性要求也会提升，往往需要根据业务或者工作负载的需求来灵活扩展计算或者存储，如果采用计算与存储紧耦合的方式，计算与存储扩缩容则极为不方便，无法满足用户业务对于灵活性的需求
[^_^]:
    支持存储即计算的架构设计，计算与存储融合部署,共享硬件资源，降低系统成本；同时提供数据位置感知、数据布局可控等功能，支持基于数据分布特征的高效数据处理模式（这个设计我认为不合理， 注释掉）

**超高单流速度**
本地优先读写、客户端磁盘缓存、元数据内存优化等多种单流读写性能优化方法，有效降低访问延迟,提升读写速度，解决传统计算程序中单线程数据采集与访问的性能问题。


## 海量小文件场景

### 背景
**文件检索困难**
海量小文件环境下，元数据操作比例大幅上升，大量并发的随机磁盘访问,极大降低了机械磁盘运行效率，导致文件检索速度极差，甚至会出现大目录无法打开的情况。

**读写性能低下**
海量小文件环境下，由于文件较小，数据读写也同样表现为随机模式，数据访问延迟较大，而用户业务多采用同步方式处理海量文件 ,系统整体的IOPS表现极低，业务操作耗时居高不下。

**运维困难**
管理海量文件困难众多，备份数据、删除文件、文件整理等日常运维操作，都会相当耗时，甚至会持续数天。同时，这些操作也消耗大量系统资源，影响前端业务的正常运转。

**成本问题**
某些高端存储设备采用大容量缓存和大量SSD磁盘来提升海量小文件性能，但是成本较高。另外,业务中经常出现的小文件及大文件混合应用的场景，也进一步降低了产品性价比。

### 解决方案
**元数据性能优化设计**
文件数据与元数据分离存储和管理，各自专门优化。元数据服务器基于内存实现，采用share-nothing的集群机制，突破元数据性能瓶颈，支持百亿级别文件总数目和千万级单目录文件数目。

**前后端SSD性能加速**
数据服务器采用策略存储机制，使用SSD来存放小文件数据，提升小文件性能。专有客户端通过磁盘缓存功能，利用SSD构建大容量高速缓存，提高命中率，减少磁盘和网络开销。

**成本可控**
主流通用x86服务器及以太网络构建，经济可靠；灵活的存储策略可充分利用少量SSD即可达到可观性能；弹性扩展，按需购置。

**安全可靠**
高可用集群架构，数据智能冗余分布，系统无单点故障，数据实时校验，故障自动切换，数据自动恢复，确保数据安全可靠及业务连续。

### 方案亮点
**策略存储**
数据服务器支持SSD和大容量磁盘混插设计（用户自定比例），并通过独有的策略存储机制，充分利用SSD来提升小文件和热点数据性能，同时使用大容量磁盘来存储大文件及冷数据。

**客户端SSD缓存**
特有的客户端磁盘缓存功能，可利用多块SSD在客户端构建大容量高速缓存，智能透明的缓存大量热点小文件，减少多次后端网络及磁盘开销，提供最优的性能体验。

**运维支持**
提供离线文件元数据数据库支持，可通过SQL接口来简化和加速海量小文件的查询，并为业务统计提供有力支持。同时，提供了一批高效工具（如快速批量删除），减轻海量文件的运维难度。

## 大数据处理场景

### 背景
**极高性能要求**
离线处理方式已无法满足当前业务需求，实时计算、流数据处理正在成为主流，数据的采集、处理、展现均对存储性能提出极高要求；大数据处理框架通常规模巨大，处理作业大量并发，要求极高的聚合吞吐率，存储性能面临更大挑战。

**数据总量巨大**
随着互联网用户、移动终端、传感器设备等数量快速增长，需要处理的数据爆炸式增长，并随时间持续累积，例如，大型网站的用户日志数据量可达每日百TB，因此，存储系统支持PB级甚至EB级数据的高效存储是进行数据处理的前提。

**数据类型多样**
大数据处理应用类型众多，数据来源丰富、采集方式各异，数据类型涵盖结构化数据、文本、日志、图像、视频等，同时，不同大数据处理框架（如Hadoop、Storm、Spark等）对存储访问接口、数据访问模式的要求也不尽相同。

**高度扩展要求**
随着业务的不断发展，数据量持续增大，数据处理规模的扩展在所难免，大数据处理平台多采用分布式架构，计算力的提升可通过扩展处理节点实现，存储系统也应具有在线扩展能力，可按需匹配数据量攀升导致的存储容量和性能需求。

### 解决方案
**高可扩展架构**
模块化Scale-out架构，容量和性能均可通过节点堆叠而线性扩展，系统节点数目可在线平滑扩充至数千台， 提供EB级存储容量和数百GB聚合I/O带宽，有效支撑大数据存储与处理。

**匹配性能需求**
存储系统性能随节点数目扩展，高效应对海量数据的大规模并发处理；可与主流大数据处理框架整合部署，构建存储/计算一体化处理平台，共享硬件资源，优化数据读写，消除存储网络瓶颈，数据处理性能倍增。

**丰富访问接口**
提供HDFS插件，完美对接Hadoop/Spark /Storm等数据处理平台，以及Flume/Chukwa等数据采集工具；支持多种语言的编程接口, 高效对接数据处理应用；提供多种接口的POSIX文件访问, 同时兼容传统数据处理应用。

**通用硬件平台**
采用普通商用服务器、主流互连网络搭建，无需昂贵的专有设备; 支持高速SSD与低成本大容量SATA/SAS磁盘混插, 兼顾性能与成本；与主流大数据处理框架的硬件平台需求相同，可整合部署，降低总体拥有成本。

### 方案亮点

**HDFS插件**
实现了基于原生Java API之上的HDFS插件，完美模拟HDFS系统，针对应用特别优化，解决HDFS性能问题，可高效支持各类Hadoop应用。

**分布式集群架构**
有别于传统存储系统，与主流分布式大数据处理框架采用相同设计原理，均为基于普通硬件的集群式架构，可实现系统级整合，具有超高扩展能力。

**成本可控**
通用硬件平台、主流网络支持，大幅降低构建成本；模块式横向扩展，扩展成本低廉可控；存储即计算的融合架构，硬件资源复用，数据处理系统成本骤降。


## 视频监控场景
### 背景
### 解决方案
### 方案亮点

## 医疗影像场景
### 背景
### 解决方案
### 方案亮点

## 媒体融合场景
### 背景
### 解决方案
### 方案亮点

## 高清制作场景
### 背景
### 解决方案
### 方案亮点

## 媒资管理场景
### 背景
### 解决方案
### 方案亮点

## 集群渲染场景
### 背景
### 解决方案
### 方案亮点
