
# 竞品的产品架构
    他们的硬件架构是怎么样的
    他们的软件架构是怎么样的
    他们基于社区版进行了那些改进, 标书或者产品介绍里一般会有体现强调他们的优势
    他们架构中基于开发改进或调优了哪些瓶颈

# ceph优化的几个方向

## 数据分布不均衡
这个问题的根本原因在于 Ceph 中映射关系完全靠固定的计算得到。当用户选定好对象名（算法主要变化的入参就是对象名称）后，其存放位置就已经基本确定下来；而用户选择什么样的名称是存储系统不可控的。
        
### 对象名称生成器
Ceph 的一大开创性设计是其CRUSH算法，可以不需要查表，仅通过计算得到 Rados 对象的存储位置。这虽然省去了元数据表的消耗，但也为 Ceph 带来了经常被诟病的问题，即数据分布不均衡（因为完全依赖于哈希函数的随机性）。在 NOS 原有的存储引擎中，互为副本的硬盘成对使用，每块硬盘仅需保留数十 GiB 左右的空间作缓冲即可，对于对象存储中动辄上 10TB 的硬盘这完全可以接受，其最终的容量使用率可以达到 99%甚至以上。而 Ceph 在一个存储池中某块盘达到 full ratio（默认为 95%）时就会限制整个池的业务写入（因为无法预测到新数据是否会哈希到这块盘上），这时同一个池中使用率最低的硬盘可能仅用了 60% ～ 70%，导致整体容量利用率往往只能维持在 80%左右，这无疑是一种巨大的浪费。虽然 Ceph 在较新的版本中提供了一定的自均衡功能，然而其会引发大量的数据迁移，代价依然不小。

这个问题的根本原因在于 Ceph 中映射关系完全靠固定的计算得到。当用户选定好对象名（算法主要变化的入参就是对象名称）后，其存放位置就已经基本确定下来；而用户选择什么样的名称是存储系统不可控的。NOS 在实现对象合并功能时，使用 DDB 记录了 NOS 对象到 Rados 对象的映射关系；换一个角度看，这相当于在业务数据和 Ceph 系统之间增加了一层映射隔离，带来的好处是我们可以自主地选择 Rados 对象名称，从而控制数据存放的位置。因此，在 LifeCycle 程序中我们又加入了对象名称生成功能，提前准备好合适的对象名，确保该对象能尽可能地写到使用率较低的硬盘上。另外，对于那些已经达到 full ratio 阈值的硬盘，可以选择将其过滤，然后继续往剩下的盘中写入。在上线此功能后，NOS 的硬盘容量平均使用率可以提升到 97%左右，大大节省了成本

## 小对象
元数据的承载。
海量小文件处理的瓶颈在于对元数据的处理，业内通常采用分布式数据库实现。通过对元数据进行独立组织与承载，并通过元数据语义优化、写入优化等，降低元数据在IO路径和资源等方面不必要的性能消耗与写入次数。匹配上优化的技术，减少IO数量，比如在处理业务高并发的时候，将并发的多个操作合并成一个操作，进一步提升吞吐。最后，为了进一步保障元数据的小IO高性能，通常将元数据存储在SSD的数据分层空间中，进一步加速元数据的访问效率。

分布式智能缓存技术
针对海量小文件设计的分布式智能缓存层，能够让小文件在写入SSD后即返回，缩短IO路径，有效降低时延，提高性能。同时还可以有效降低原生纠删码的I/O写入放大的问题，提高原生纠删码的性能，进一步提升分布式存储对海量小文件的性能支持。

小文件合并
通过将小文件落在智能缓存的同时还能够将小文件在线合并成大IO，然后通过条带化技术（将大数据切分成小数据并发存储到不同硬盘）写入HDD，极大地提升了IO的性能。并且小文件合并还能够减少文件数量，从而减少对应的元数据数量，来提升性能。

## 硬件资源优化
对于高密度的存储机器而言，硬件成本（主要是内存和 CPU）通常抠算的很紧，因此如何在有限的资源下提供更高更稳定的性能一直是我们钻研的重点之一。在深入分析开源代码后，我们对其做了“大瘦身”，
删去了 Rados 中一些不必要的功能（例如快照），简化了关键结构体（例如 Onode）的缓存形态，仅保留 对象存储 真正需要的流程和信息。
目前为止，在相同硬件配置下 对象存储 Rados 相较开源版本读写性能提升均在 30%以上。

## 对存储系统分级
首先，在存储集群当中，出于对访问性能、成本等因素的考虑，我们可能会同时引入 SSD 和 HDD。
在这种情况下，如果不进行存储分级，就可能会导致某些对访问性能要求不高的数据，或是归档数据，被存储在 SSD 中，而某些对访问性能要求较高的数据则被存储在了 HDD 中，
这无疑会影响数据的访问性能，同时也提高了数据的存储成本。

## 存储策略
3 副本
2 副本
Erasure Code

有的数据对可靠性要求很高，我们才会将其以三副本的形式进行存储。可能有的数据，我们对它的可靠性要求没那么高，那我们可以考虑将其以两副本的形式进行存储，节省存储空间

## ceph社区未来规划
Ceph传统的线程模型是多线程+队列的模型，一个IO从发起到完成要经历重重队列和不同的线程池，锁竞争、上下文切换和Cache Miss比较严重，也导致IO延迟迟迟降不下来。通过Perf发现CPU主要都耗在了锁竞争和系统调用上，Ceph自身的序列化和反序列化也比较消耗CPU
所以需要一套新的编程框架来解决上述问题。Seastar是一套基于future-promsie现代化高效的share-nothing的网络编程框架，从18年开始，Ceph社区便基于Seastar来重构整个OSD，项目代号Crimson，来更好的解决上述问题。

### Crimson设计目标

    最小化CPU开销。
    减少跨核通信。
    减少数据拷贝。
    Bypass Kernel，减少上下文切换。
    支持新硬件：ZNS-NVME、PMEM等。

线程模型

![线程模型](/img/v2-c146d302ca530935963385af249bcbf1_720w.jpg "Magic Gardens")

### 性能对比
测试RBD时，在达到同等iops和延迟时，crimson-osd的cpu比ceph-osd的cpu少了好几倍。

### BlueStore适配

BlueStore目前是Ceph里性能比较高的单机存储引擎，从设计研发到稳定差不多持续了3年时间，足以说明研发一个单机存储引擎的时间成本是比较高的。由于BlueStore不符合Seastar的编程模型，所以需要对BlueStore适配，目前有两种方案：

BlueStore-Alien：使用一个Alien Thread，使用Seastar的编程模型专门向Seastar-Reactor提交BlueStore的任务。
BlueStore-Native：使用Seastar-Env来实现RocksDB的Rocksdb-Env，从而更原生的适配。
但是由于RocksDB有自己的线程模型，外部不可控，所以无论怎么适配都不是最好的方案，理论上从0开始用基于Seastar的模型来写一个单机存储引擎是最完美的方案，于是便有了SeaStore，而BlueStore的适配也作为中间过渡方案，最多可用于HDD。

### SeaStore

SeaStore是下一代的ObjectStore，适用于Crimson的后端存储，专门为了NVME设计，使用SPDK访问，同时由于Flash设备的特性，重写时必须先要进行擦除操作，也就是内部需要做GC，是不可控的，所以Ceph希望把Flash的GC提到SeaStore中来做：

SeaStore的逻辑段(segment)理想情况下与硬件segment(Flash擦除单位)对齐。
SeaStar是每个线程一个CPU核，所以将底层按照CPU核进行分段，每个核分配指定个数的segment。
当磁盘利用率达到阈值时，将少量的GC清理工作和正常的写流量一起做。
元数据使用B+数存储，而不是原来的RocksDB。
所有segment都是追加顺序写入的。

引用：https://zhuanlan.zhihu.com/p/360355168


## io_uring(相当于windows的iocp)
Linux内核5.1支持了新的异步IO框架iouring，由Block IO大神也即Fio作者Jens Axboe开发，意在提供一套公用的网络和磁盘异步IO，不过io_uring目前在磁盘方面要比网络方面更加成熟。
通过liburing使用io_uring, 在 polling 模式下，io_uring 和 SPDK 的性能非常接近，特别是高 QueueDepth 下，io_uring 有赶超的架势，同时完爆 libaio。
ceph的io_uring主要使用在block_device，抽象出了统一的块设备，直接操作裸设备，对上层提供统一的读写方法。bluefs仅仅需要提供append only的写入即可，不需要提供随机写，大大简化了bluefs的实现。

SPDK与io_uring新异步IO机制，在其抽象的通用块层加入了io_uring的支持。
引用：https://zhuanlan.zhihu.com/p/361955546

## 在支撑一些延迟敏感的在线应用过程中，我们发现 Ceph 的尾延迟较差
当应用并发负载较高时，Ceph 很容易出现延迟的毛刺，对延迟敏感的应用造成超时甚至崩溃。我们对 Ceph 的尾延迟问题进行了深入细致的分析和优化。造成尾延迟的一个重要原因就是代码中锁的使用问题

### 持锁时间过长
#### 异步读优化
#### 锁粒度过粗
#### object cache lock 优化
### 不必要的锁竞争
#### 减少 pg lock 竞争
#### log lock 优化
#### filestore apply lock 优化

引用：https://www.infoq.cn/article/Jv3GygMyVBgedrN84rAT


## ceph-fuse io 性能优化
问题现象：
某用户在使用 dd 命令：dd if=/dev/zero of=./test2 bs=1M count=100 oflag=direct 写一个文件的时候 io 性能只能达到 30MB/s，而本地文件系统可以达到 100MB/s 的写入性能，差距较大。
原因是fuse 内核态代码(fuse.ko)以及 libfuse 都有对 IO 的限制，当 IO 大于 128k 的时候会进行拆分同步下发

问题分析：
1.我们调整了 dd 的 bs 参数设置到 2M,256K,128K,64K 分别进行了测试
2.发现当 bs 小于 128k 的时候，带宽会变小，bs 大于或等于 128k 的时候带宽始终不变。
3.我们分析 fuse 内核态代码(fuse.ko)以及 libfuse 都有对 IO 的限制，当 IO 大于 128k 的时候会进行拆分同步下发。

问题解决：
1.修改了对应的 fuse 限制相关的代码，加载新的 fuse.ko 以及 libfuse。
2.重新测试 dd 命令，在 bs=1M 的时候性能从 30MB/s 提升到 120MB/s。

## CephFS 空间回收优化
某用户反馈在 PV 中实际数据只有几百 G，但是通过监控显示实际占用 10 多 T 的空间，使用率已经达到 70%以上，如下图。
问题分析：
1.通过和用户沟通，了解到大致的应用场景是持续不断的写文件以及删除老文件。
2.而我们通过水位的监控看到容量一直在持续增长。
3.通过 Ceph 的工具查看到回收站中待删除文件达到 3 万多，但是并无文件被删。
4.通过 Ceph 的命令工具发现有几十个 client 连接着 ceph-mds。个别 client 拥有几万的 caps(文件句柄)。由此可知文件虽然被某个 client 删除，但是其他的 client 还未将文件关闭，导致文件一直处于待删除状态。

问题解决：
1.推动用户将有问题的 client 文件句柄进行释放，则触发文件删除，最后集群容量降下来了。

# 如何实现数据迁移时间的可控性

    1. AWS S3 对象生命周期管理
        通过为存储桶设置生命周期管理规则，可以对存储桶中特定的对象集进行生命周期管理。

        迁移处理，即支持在经过指定的时间间隔后，或是到达某一特定时间点时，将存储桶中的特定对象集由当前的 storage class 存储类别迁移到另外一个指定的 storage class 存储类别中；
        过期删除处理，即支持在经过指定的时间间隔后，或是到达某一特定时间点时，将存储桶中的特定对象集进行清除。

    2. RGW 对象生命周期管理
        当前，Ceph RGW 对象存储实际上也支持 LC 对象生命周期管理。但是，因为 RGW 本身并不支持 object storage class / placement rule，
        因此其对象生命周期管理目前只支持 Expiration actions 过期删除处理。


        实现完整的对象生命周期管理
            基于上面实现的 Object Storage Class，在 RGW 现有 LC 实现的基础上，我们对 RGW LC 的处理逻辑进行了扩展，实现了 LC 迁移功能，支持通过对象生命周期管理，将对象数据迁移到其他存储类别 storage class 中，例如支持从 SSD 迁移到 HDD，从 3 副本池迁移到 2 副本池，从副本池迁移到纠删码池，从 Ceph 集群中迁移到外部 Ufile 公有云存储等等，从而实现了完整的对象生命周期管理。支持标准的 AWS S3 ObjectLifecycle Management 的相关接口。
            由上面的介绍，我们实现的 Storage Class 功能是支持将外部存储指定为一个存储类别的，因此，支持通过配置存储桶的 LC 规则，将该存储桶中的某一特定对象集迁移到外部存储中，如 UFile、S3 等等。
            相较于 RGW 的 Cloud Sync 功能，通过配置 LC 迁移规则将 Ceph 集群中的对象数据迁移到外部云存储具有如下优点：
            操作的粒度更细，可以直接以对象为单位，对数据进行操作；
            时间可控，可以通过在 LC 规则当中对操作生效的时间进行配置指定，人为控制数据迁移的时间，时间可控性更强；
            至此，我们已经在 Ceph 对象存储的基础上，实现了一套完整的、全粒度支持的数据迁移处理机制，从 zone 级、到 bucket 级、再到 object 级、基本可以覆盖所有应用场景的常见需求。

    3. 自动生成迁移策略
        存储桶日志
            存储桶日志是用于记录追踪对某一特定存储桶的操作和访问的功能特性。存储桶日志的每条日志记录都记录了一次对相应存储桶的操作访问请求的细节，例如请求的发起者、存储桶名字、请求时间、请求的操作、返回的状态码等等。

        自动生成迁移策略
            根据存储桶日志中的操作记录、以及可配置的标尺参数，对存储桶中的对象数据的热度进行分析，并按照分析结果自动生成迁移策略，对对象数据进行管理。一张图来概要介绍下处理流程：


    关于未来
        基于 Ceph 对象存储的分级混合云存储方案能够很好的满足使用者的需求，但是在支持数据双向同步、代理读写等功能上还要继续完善

引用：
https://www.infoq.cn/article/OZEKP2EHjv9jjokEcz9Q?utm_source=related_read_bottom&utm_medium=article


# Ceph 现有架构与业务存在哪些问题？
![现有架构与业务存在哪些问题](/img/eb9581b9302d3f3cc90f5ac42dae0d66.png "Magic Gardens")

## Ceph 数据与内核缓存的割裂问题。
以 BlueStore 为例，它将 OSD 的元数据保存在 RockDB 中，RockDB 又是运行在一个简化的文件系统(Blue FS)之上的，此时可以认为 Blue FS 的 IO 就是 OSD 的元数据，但对于内核缓存来说，它是无法感知 OSD 下发的数据是有元数据和业务数据之分的。

## 内核缓存无法区分 OSD 业务的热点数据和冷数据
比如用户配置比较典型的三副本存储策略，此时只有主副本数据才会为客户端提供读写服务，而从副本则直接不用缓存即可，但是内核层缓存是没法区分这种差异的。如果使用的是 DM Cache，还会在内存中分配一个空间来缓存部分数据，这无还疑会浪费内存资源。
总体来说，内核缓存存在浪费 Cache 空间，还有 Cache 命中率不高，频繁刷新缓存导致缓存设备寿命缩短等缺陷。

## 解决方案？
![方案](/img/3cb1c649fce452683f355b9304f770a9.png "Magic Gardens")

在 BlueStore 下发 IO 的地方增加一个适配层，用来标记下发 IO 的类型，在内核缓存入口处增加一个适配层，用来捕捉 OSD 的 IO 类型，最后在内核缓存处理时，根据 IO 类型进行不同的写回、淘汰策略。

例如将三副本中的两个从副本用 NOCACHE 标签经过适配层随 IO 请求一起带到内核缓存里去，这样内核缓存就可以不缓存，直接写后备盘。同时可以将 Blue FS 的 IO 标记成元数据类型，让其在内核缓存更长时间的驻留。或根据用户业务需求，将用户有比较重要的，且读写频繁的数据，标为高等级，其他不重要的数据标成中或低等级，然后在内核缓存针对高等级数据采用和元数据一样的处理策略；这样就可以根据用户需求来执行不同的淘汰和回写策略。

![方案](/img/7aa9e85daf64a25d6a33e5b2783298fa.png "Magic Gardens")

BlueStore 使用的是 Libaio API 接口来下发 IO 请求，此时需要一个 IOCB 结构体作为下发请求的 IO 参数，可以通过 io_prep_pwrite 函数生成 iocb 结构体之后，把 io flag 设置到 IOCB 的 Flag 结构体当中，作为 io_submit 的参数，一起提交至内核层；内核在 VFS 层时捕捉到 Direct io 时，将 flag 转换到通用块设备层的 BIO 结构体里面，比如 BIO 的 bi_rw 结构体里面，此时位于通用块层的内核缓存即可捕捉到上层业务标签。内核缓存可根据不同的标签执行不同的回写、分配策略，比如让元数据更久的驻留在缓存当中，或者让高等级的用户数据和元数据执行相同的缓存策略。


        